# models/vlm_short.py

import sys
import os

# sys.pathì— ShortVLM ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# mobilevlm_official í´ë” ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../mobilevlm_official")))

from mobilevlm_runtime import MobileVLMRuntime
from transformers import pipeline
from deep_translator import GoogleTranslator
from gtts import gTTS
import time
import os


# ë²ˆì—­ê¸° ì´ˆê¸°í™” (1íšŒë§Œ)
translator_ko2en = pipeline("translation", model="Helsinki-NLP/opus-mt-ko-en")  # Hugging Face íŒŒì´í”„ë¼ì¸
translator_en2ko = GoogleTranslator(source='en', target='ko')  # êµ¬ê¸€ ë²ˆì—­ê¸°

# VLM ëª¨ë¸ ì´ˆê¸°í™” (1íšŒë§Œ)
runtime = MobileVLMRuntime()

def run_once(image_path: str, kor_command: str):
    start_total = time.time()

    # Step 1: í•œâ†’ì˜ ë²ˆì—­
    translated = translator_ko2en(kor_command, max_length=60)[0]["translation_text"]
    prompt = f""" '{translated}' """

    # Step 2: ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
    result, duration = runtime.run(image_path, prompt)

    # Step 3: ì˜â†’í•œ ë²ˆì—­
    translated_back = translator_en2ko.translate(result)

    # # Step 4: TTS ì €ì¥ ë° ì¬ìƒ
    # tts = gTTS(translated_back, lang='ko')
    # tts.save("output.mp3")
    # os.system("mpg123 output.mp3")

    end_total = time.time()
    total_time = end_total - start_total

    # Step 5: ì¶œë ¥
    print(f"\nğŸ–¼ï¸ Image: {image_path}")
    print(f"ğŸ—£ï¸ Korean Prompt: {kor_command}")
    print(f"ğŸ”¤ English Prompt: {prompt}")
    print(f"ğŸ“¢ Output (EN): {result}")
    print(f"ğŸ“¢ Output (KO): {translated_back}")
    print(f"â±ï¸ Inference Time: {duration:.2f} seconds (Model only)")
    print(f"â±ï¸ Total Time: {total_time:.2f} seconds (Translation + Model)")

if __name__ == "__main__":
    # ì˜ˆì‹œ ì‹¤í–‰
    run_once("test/test_images/test1.jpg", "ì•ì— ë­ ìˆì–´?")
